## review

1. Qiu X, Sun T, Xu Y, et al. Pre-trained models for natural language processing: A survey[J]. Science China Technological Sciences, 2020, 63(10): 1872-1897. [issue link](https://github.com/eleveyuan/PR/issues/1)

## basic deep language model

1. Bengio Y, Ducharme R, Vincent P. A neural probabilistic language model[C]//Advances in Neural Information Processing Systems. 2001: 932-938. [issue link](https://github.com/eleveyuan/PR/issues/2)
2. Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155.
3. Bengio Y, Senécal J S. Quick training of probabilistic neural nets by importance sampling[C]//International Workshop on Artificial Intelligence and Statistics. PMLR, 2003: 17-24. [issue link](https://github.com/eleveyuan/PR/issues/4)
4. Morin F, Bengio Y. Hierarchical probabilistic neural network language model[C]//International workshop on artificial intelligence and statistics. PMLR, 2005: 246-252. [issue link](https://github.com/eleveyuan/PR/issues/6)
5. Bengio Y. New distributed probabilistic language models[J]. 2002. [issue link](https://github.com/eleveyuan/PR/issues/5)

## word2vec
1. Rong X. word2vec parameter learning explained[J]. arXiv preprint arXiv:1411.2738, 2014. [issue link](https://github.com/eleveyuan/PR/issues/10) [源码阅读](https://github.com/eleveyuan/gist_reading/tree/master/c/word2vec#word2vec)
2. Levy O, Goldberg Y, Dagan I. Improving distributional similarity with lessons learned from word embeddings[J]. Transactions of the association for computational linguistics, 2015, 3: 211-225. [issue link](https://github.com/eleveyuan/PR/issues/15)

## GloVe
global vector

1. Huang E H, Socher R, Manning C D, et al. Improving word representations via global context and multiple word prototypes[C]//Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2012: 873-882. [issue link](https://github.com/eleveyuan/PR/issues/13)
2. Pennington J, Socher R, Manning C D. Glove: Global vectors for word representation[C]//Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014: 1532-1543. [issue link](https://github.com/eleveyuan/PR/issues/12)
3. Arora S, Li Y, Liang Y, et al. A latent variable model approach to pmi-based word embeddings[J]. Transactions of the Association for Computational Linguistics, 2016, 4: 385-399.
4. Arora S, Li Y, Liang Y, et al. Linear algebraic structure of word senses, with applications to polysemy[J]. Transactions of the Association for Computational Linguistics, 2018, 6: 483-495. [issue link](https://github.com/eleveyuan/PR/issues/14)

## fasttext
1. Bojanowski P, Grave E, Joulin A, et al. Enriching word vectors with subword information[J]. Transactions of the association for computational linguistics, 2017, 5: 135-146. [issue link](https://github.com/eleveyuan/PR/issues/16) [源码阅读](https://github.com/eleveyuan/gist_reading/tree/master/c/fasttext)
2. Joulin A, Grave E, Bojanowski P, et al. Bag of tricks for efficient text classification[J]. arXiv preprint arXiv:1607.01759, 2016. 
3. Joulin A, Grave E, Bojanowski P, et al. Fasttext. zip: Compressing text classification models[J]. arXiv preprint arXiv:1612.03651, 2016.

## transformer
1. Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.
