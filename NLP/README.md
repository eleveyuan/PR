## review

1. Qiu X, Sun T, Xu Y, et al. Pre-trained models for natural language processing: A survey[J]. Science China Technological Sciences, 2020, 63(10): 1872-1897. [issue link](https://github.com/eleveyuan/PR/issues/1)

## basic deep language model

1. Bengio Y, Ducharme R, Vincent P. A neural probabilistic language model[C]//Advances in Neural Information Processing Systems. 2001: 932-938. [issue link](https://github.com/eleveyuan/PR/issues/2)
2. Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155.
3. Bengio Y, Sen√©cal J S. Quick training of probabilistic neural nets by importance sampling[C]//International Workshop on Artificial Intelligence and Statistics. PMLR, 2003: 17-24. [issue link](https://github.com/eleveyuan/PR/issues/4)
4. Morin F, Bengio Y. Hierarchical probabilistic neural network language model[C]//International workshop on artificial intelligence and statistics. PMLR, 2005: 246-252. [issue link](https://github.com/eleveyuan/PR/issues/6)
5. Bengio Y. New distributed probabilistic language models[J]. 2002. [issue link](https://github.com/eleveyuan/PR/issues/5)
6. Rong X. word2vec parameter learning explained[J]. arXiv preprint arXiv:1411.2738, 2014.
